{"ast":null,"code":"'use strict';\n\nconst BB = require('bluebird');\n\nconst contentPath = require('./content/path');\n\nconst figgyPudding = require('figgy-pudding');\n\nconst finished = BB.promisify(require('mississippi').finished);\n\nconst fixOwner = require('./util/fix-owner');\n\nconst fs = require('graceful-fs');\n\nconst glob = BB.promisify(require('glob'));\n\nconst index = require('./entry-index');\n\nconst path = require('path');\n\nconst rimraf = BB.promisify(require('rimraf'));\n\nconst ssri = require('ssri');\n\nBB.promisifyAll(fs);\nconst VerifyOpts = figgyPudding({\n  concurrency: {\n    default: 20\n  },\n  filter: {},\n  log: {\n    default: {\n      silly() {}\n\n    }\n  }\n});\nmodule.exports = verify;\n\nfunction verify(cache, opts) {\n  opts = VerifyOpts(opts);\n  opts.log.silly('verify', 'verifying cache at', cache);\n  return BB.reduce([markStartTime, fixPerms, garbageCollect, rebuildIndex, cleanTmp, writeVerifile, markEndTime], (stats, step, i) => {\n    const label = step.name || \"step #\".concat(i);\n    const start = new Date();\n    return BB.resolve(step(cache, opts)).then(s => {\n      s && Object.keys(s).forEach(k => {\n        stats[k] = s[k];\n      });\n      const end = new Date();\n\n      if (!stats.runTime) {\n        stats.runTime = {};\n      }\n\n      stats.runTime[label] = end - start;\n      return stats;\n    });\n  }, {}).tap(stats => {\n    stats.runTime.total = stats.endTime - stats.startTime;\n    opts.log.silly('verify', 'verification finished for', cache, 'in', \"\".concat(stats.runTime.total, \"ms\"));\n  });\n}\n\nfunction markStartTime(cache, opts) {\n  return {\n    startTime: new Date()\n  };\n}\n\nfunction markEndTime(cache, opts) {\n  return {\n    endTime: new Date()\n  };\n}\n\nfunction fixPerms(cache, opts) {\n  opts.log.silly('verify', 'fixing cache permissions');\n  return fixOwner.mkdirfix(cache, cache).then(() => {\n    // TODO - fix file permissions too\n    return fixOwner.chownr(cache, cache);\n  }).then(() => null);\n} // Implements a naive mark-and-sweep tracing garbage collector.\n//\n// The algorithm is basically as follows:\n// 1. Read (and filter) all index entries (\"pointers\")\n// 2. Mark each integrity value as \"live\"\n// 3. Read entire filesystem tree in `content-vX/` dir\n// 4. If content is live, verify its checksum and delete it if it fails\n// 5. If content is not marked as live, rimraf it.\n//\n\n\nfunction garbageCollect(cache, opts) {\n  opts.log.silly('verify', 'garbage collecting content');\n  const indexStream = index.lsStream(cache);\n  const liveContent = new Set();\n  indexStream.on('data', entry => {\n    if (opts.filter && !opts.filter(entry)) {\n      return;\n    }\n\n    liveContent.add(entry.integrity.toString());\n  });\n  return finished(indexStream).then(() => {\n    const contentDir = contentPath._contentDir(cache);\n\n    return glob(path.join(contentDir, '**'), {\n      follow: false,\n      nodir: true,\n      nosort: true\n    }).then(files => {\n      return BB.resolve({\n        verifiedContent: 0,\n        reclaimedCount: 0,\n        reclaimedSize: 0,\n        badContentCount: 0,\n        keptSize: 0\n      }).tap(stats => BB.map(files, f => {\n        const split = f.split(/[/\\\\]/);\n        const digest = split.slice(split.length - 3).join('');\n        const algo = split[split.length - 4];\n        const integrity = ssri.fromHex(digest, algo);\n\n        if (liveContent.has(integrity.toString())) {\n          return verifyContent(f, integrity).then(info => {\n            if (!info.valid) {\n              stats.reclaimedCount++;\n              stats.badContentCount++;\n              stats.reclaimedSize += info.size;\n            } else {\n              stats.verifiedContent++;\n              stats.keptSize += info.size;\n            }\n\n            return stats;\n          });\n        } else {\n          // No entries refer to this content. We can delete.\n          stats.reclaimedCount++;\n          return fs.statAsync(f).then(s => {\n            return rimraf(f).then(() => {\n              stats.reclaimedSize += s.size;\n              return stats;\n            });\n          });\n        }\n      }, {\n        concurrency: opts.concurrency\n      }));\n    });\n  });\n}\n\nfunction verifyContent(filepath, sri) {\n  return fs.statAsync(filepath).then(stat => {\n    const contentInfo = {\n      size: stat.size,\n      valid: true\n    };\n    return ssri.checkStream(fs.createReadStream(filepath), sri).catch(err => {\n      if (err.code !== 'EINTEGRITY') {\n        throw err;\n      }\n\n      return rimraf(filepath).then(() => {\n        contentInfo.valid = false;\n      });\n    }).then(() => contentInfo);\n  }).catch({\n    code: 'ENOENT'\n  }, () => ({\n    size: 0,\n    valid: false\n  }));\n}\n\nfunction rebuildIndex(cache, opts) {\n  opts.log.silly('verify', 'rebuilding index');\n  return index.ls(cache).then(entries => {\n    const stats = {\n      missingContent: 0,\n      rejectedEntries: 0,\n      totalEntries: 0\n    };\n    const buckets = {};\n\n    for (let k in entries) {\n      if (entries.hasOwnProperty(k)) {\n        const hashed = index._hashKey(k);\n\n        const entry = entries[k];\n        const excluded = opts.filter && !opts.filter(entry);\n        excluded && stats.rejectedEntries++;\n\n        if (buckets[hashed] && !excluded) {\n          buckets[hashed].push(entry);\n        } else if (buckets[hashed] && excluded) {// skip\n        } else if (excluded) {\n          buckets[hashed] = [];\n          buckets[hashed]._path = index._bucketPath(cache, k);\n        } else {\n          buckets[hashed] = [entry];\n          buckets[hashed]._path = index._bucketPath(cache, k);\n        }\n      }\n    }\n\n    return BB.map(Object.keys(buckets), key => {\n      return rebuildBucket(cache, buckets[key], stats, opts);\n    }, {\n      concurrency: opts.concurrency\n    }).then(() => stats);\n  });\n}\n\nfunction rebuildBucket(cache, bucket, stats, opts) {\n  return fs.truncateAsync(bucket._path).then(() => {\n    // This needs to be serialized because cacache explicitly\n    // lets very racy bucket conflicts clobber each other.\n    return BB.mapSeries(bucket, entry => {\n      const content = contentPath(cache, entry.integrity);\n      return fs.statAsync(content).then(() => {\n        return index.insert(cache, entry.key, entry.integrity, {\n          metadata: entry.metadata,\n          size: entry.size\n        }).then(() => {\n          stats.totalEntries++;\n        });\n      }).catch({\n        code: 'ENOENT'\n      }, () => {\n        stats.rejectedEntries++;\n        stats.missingContent++;\n      });\n    });\n  });\n}\n\nfunction cleanTmp(cache, opts) {\n  opts.log.silly('verify', 'cleaning tmp directory');\n  return rimraf(path.join(cache, 'tmp'));\n}\n\nfunction writeVerifile(cache, opts) {\n  const verifile = path.join(cache, '_lastverified');\n  opts.log.silly('verify', 'writing verifile to ' + verifile);\n\n  try {\n    return fs.writeFileAsync(verifile, '' + +new Date());\n  } finally {\n    fixOwner.chownr.sync(cache, verifile);\n  }\n}\n\nmodule.exports.lastRun = lastRun;\n\nfunction lastRun(cache) {\n  return fs.readFileAsync(path.join(cache, '_lastverified'), 'utf8').then(data => new Date(+data));\n}","map":null,"metadata":{},"sourceType":"script"}