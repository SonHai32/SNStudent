{"ast":null,"code":"'use strict';\n\nvar _taggedTemplateLiteral = require(\"/home/hari/Desktop/web-dev/React/cstudy/node_modules/@babel/runtime/helpers/taggedTemplateLiteral\");\n\nfunction _templateObject() {\n  const data = _taggedTemplateLiteral([\"No cache entry for `\", \"` found in `\", \"`\"], [\"No cache entry for \\\\`\", \"\\\\` found in \\\\`\", \"\\\\`\"]);\n\n  _templateObject = function () {\n    return data;\n  };\n\n  return data;\n}\n\nconst BB = require('bluebird');\n\nconst contentPath = require('./content/path');\n\nconst crypto = require('crypto');\n\nconst figgyPudding = require('figgy-pudding');\n\nconst fixOwner = require('./util/fix-owner');\n\nconst fs = require('graceful-fs');\n\nconst hashToSegments = require('./util/hash-to-segments');\n\nconst ms = require('mississippi');\n\nconst path = require('path');\n\nconst ssri = require('ssri');\n\nconst Y = require('./util/y.js');\n\nconst indexV = require('../package.json')['cache-version'].index;\n\nconst appendFileAsync = BB.promisify(fs.appendFile);\nconst readFileAsync = BB.promisify(fs.readFile);\nconst readdirAsync = BB.promisify(fs.readdir);\nconst concat = ms.concat;\nconst from = ms.from;\nmodule.exports.NotFoundError = class NotFoundError extends Error {\n  constructor(cache, key) {\n    super(Y(_templateObject(), key, cache));\n    this.code = 'ENOENT';\n    this.cache = cache;\n    this.key = key;\n  }\n\n};\nconst IndexOpts = figgyPudding({\n  metadata: {},\n  size: {}\n});\nmodule.exports.insert = insert;\n\nfunction insert(cache, key, integrity, opts) {\n  opts = IndexOpts(opts);\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size: opts.size,\n    metadata: opts.metadata\n  };\n  return fixOwner.mkdirfix(cache, path.dirname(bucket)).then(() => {\n    const stringified = JSON.stringify(entry); // NOTE - Cleverness ahoy!\n    //\n    // This works because it's tremendously unlikely for an entry to corrupt\n    // another while still preserving the string length of the JSON in\n    // question. So, we just slap the length in there and verify it on read.\n    //\n    // Thanks to @isaacs for the whiteboarding session that ended up with this.\n\n    return appendFileAsync(bucket, \"\\n\".concat(hashEntry(stringified), \"\\t\").concat(stringified));\n  }).then(() => fixOwner.chownr(cache, bucket)).catch({\n    code: 'ENOENT'\n  }, () => {// There's a class of race conditions that happen when things get deleted\n    // during fixOwner, or between the two mkdirfix/chownr calls.\n    //\n    // It's perfectly fine to just not bother in those cases and lie\n    // that the index entry was written. Because it's a cache.\n  }).then(() => {\n    return formatEntry(cache, entry);\n  });\n}\n\nmodule.exports.insert.sync = insertSync;\n\nfunction insertSync(cache, key, integrity, opts) {\n  opts = IndexOpts(opts);\n  const bucket = bucketPath(cache, key);\n  const entry = {\n    key,\n    integrity: integrity && ssri.stringify(integrity),\n    time: Date.now(),\n    size: opts.size,\n    metadata: opts.metadata\n  };\n  fixOwner.mkdirfix.sync(cache, path.dirname(bucket));\n  const stringified = JSON.stringify(entry);\n  fs.appendFileSync(bucket, \"\\n\".concat(hashEntry(stringified), \"\\t\").concat(stringified));\n\n  try {\n    fixOwner.chownr.sync(cache, bucket);\n  } catch (err) {\n    if (err.code !== 'ENOENT') {\n      throw err;\n    }\n  }\n\n  return formatEntry(cache, entry);\n}\n\nmodule.exports.find = find;\n\nfunction find(cache, key) {\n  const bucket = bucketPath(cache, key);\n  return bucketEntries(bucket).then(entries => {\n    return entries.reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  }).catch(err => {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  });\n}\n\nmodule.exports.find.sync = findSync;\n\nfunction findSync(cache, key) {\n  const bucket = bucketPath(cache, key);\n\n  try {\n    return bucketEntriesSync(bucket).reduce((latest, next) => {\n      if (next && next.key === key) {\n        return formatEntry(cache, next);\n      } else {\n        return latest;\n      }\n    }, null);\n  } catch (err) {\n    if (err.code === 'ENOENT') {\n      return null;\n    } else {\n      throw err;\n    }\n  }\n}\n\nmodule.exports.delete = del;\n\nfunction del(cache, key, opts) {\n  return insert(cache, key, null, opts);\n}\n\nmodule.exports.delete.sync = delSync;\n\nfunction delSync(cache, key, opts) {\n  return insertSync(cache, key, null, opts);\n}\n\nmodule.exports.lsStream = lsStream;\n\nfunction lsStream(cache) {\n  const indexDir = bucketDir(cache);\n  const stream = from.obj(); // \"/cachename/*\"\n\n  readdirOrEmpty(indexDir).map(bucket => {\n    const bucketPath = path.join(indexDir, bucket); // \"/cachename/<bucket 0xFF>/*\"\n\n    return readdirOrEmpty(bucketPath).map(subbucket => {\n      const subbucketPath = path.join(bucketPath, subbucket); // \"/cachename/<bucket 0xFF>/<bucket 0xFF>/*\"\n\n      return readdirOrEmpty(subbucketPath).map(entry => {\n        const getKeyToEntry = bucketEntries(path.join(subbucketPath, entry)).reduce((acc, entry) => {\n          acc.set(entry.key, entry);\n          return acc;\n        }, new Map());\n        return getKeyToEntry.then(reduced => {\n          for (let entry of reduced.values()) {\n            const formatted = formatEntry(cache, entry);\n            formatted && stream.push(formatted);\n          }\n        }).catch({\n          code: 'ENOENT'\n        }, nop);\n      });\n    });\n  }).then(() => {\n    stream.push(null);\n  }, err => {\n    stream.emit('error', err);\n  });\n  return stream;\n}\n\nmodule.exports.ls = ls;\n\nfunction ls(cache) {\n  return BB.fromNode(cb => {\n    lsStream(cache).on('error', cb).pipe(concat(entries => {\n      cb(null, entries.reduce((acc, xs) => {\n        acc[xs.key] = xs;\n        return acc;\n      }, {}));\n    }));\n  });\n}\n\nfunction bucketEntries(bucket, filter) {\n  return readFileAsync(bucket, 'utf8').then(data => _bucketEntries(data, filter));\n}\n\nfunction bucketEntriesSync(bucket, filter) {\n  const data = fs.readFileSync(bucket, 'utf8');\n  return _bucketEntries(data, filter);\n}\n\nfunction _bucketEntries(data, filter) {\n  let entries = [];\n  data.split('\\n').forEach(entry => {\n    if (!entry) {\n      return;\n    }\n\n    const pieces = entry.split('\\t');\n\n    if (!pieces[1] || hashEntry(pieces[1]) !== pieces[0]) {\n      // Hash is no good! Corruption or malice? Doesn't matter!\n      // EJECT EJECT\n      return;\n    }\n\n    let obj;\n\n    try {\n      obj = JSON.parse(pieces[1]);\n    } catch (e) {\n      // Entry is corrupted!\n      return;\n    }\n\n    if (obj) {\n      entries.push(obj);\n    }\n  });\n  return entries;\n}\n\nmodule.exports._bucketDir = bucketDir;\n\nfunction bucketDir(cache) {\n  return path.join(cache, \"index-v\".concat(indexV));\n}\n\nmodule.exports._bucketPath = bucketPath;\n\nfunction bucketPath(cache, key) {\n  const hashed = hashKey(key);\n  return path.join.apply(path, [bucketDir(cache)].concat(hashToSegments(hashed)));\n}\n\nmodule.exports._hashKey = hashKey;\n\nfunction hashKey(key) {\n  return hash(key, 'sha256');\n}\n\nmodule.exports._hashEntry = hashEntry;\n\nfunction hashEntry(str) {\n  return hash(str, 'sha1');\n}\n\nfunction hash(str, digest) {\n  return crypto.createHash(digest).update(str).digest('hex');\n}\n\nfunction formatEntry(cache, entry) {\n  // Treat null digests as deletions. They'll shadow any previous entries.\n  if (!entry.integrity) {\n    return null;\n  }\n\n  return {\n    key: entry.key,\n    integrity: entry.integrity,\n    path: contentPath(cache, entry.integrity),\n    size: entry.size,\n    time: entry.time,\n    metadata: entry.metadata\n  };\n}\n\nfunction readdirOrEmpty(dir) {\n  return readdirAsync(dir).catch({\n    code: 'ENOENT'\n  }, () => []).catch({\n    code: 'ENOTDIR'\n  }, () => []);\n}\n\nfunction nop() {}","map":null,"metadata":{},"sourceType":"script"}